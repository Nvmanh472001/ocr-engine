{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3097c610-0607-40a9-bf20-36112b761cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vietocr.tool.config import Cfg\n",
    "from vietocr.tool.translate import build_model\n",
    "from vietocr.model.trainer import Trainer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5f438ef-4ad4-46ca-981d-6a0a9e41469f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab': 'aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0123456789!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ ',\n",
       " 'device': 'cpu',\n",
       " 'seq_modeling': 'transformer',\n",
       " 'transformer': {'d_model': 256,\n",
       "  'nhead': 8,\n",
       "  'num_encoder_layers': 6,\n",
       "  'num_decoder_layers': 6,\n",
       "  'dim_feedforward': 2048,\n",
       "  'max_seq_length': 1024,\n",
       "  'pos_dropout': 0.1,\n",
       "  'trans_dropout': 0.1},\n",
       " 'optimizer': {'max_lr': 0.0003, 'pct_start': 0.1},\n",
       " 'trainer': {'batch_size': 32,\n",
       "  'print_every': 200,\n",
       "  'valid_every': 4000,\n",
       "  'iters': 100000,\n",
       "  'export': './weights/transformerocr.pth',\n",
       "  'checkpoint': './checkpoint/transformerocr_checkpoint.pth',\n",
       "  'log': './train.log',\n",
       "  'metrics': None,\n",
       "  'num_epochs': 100},\n",
       " 'dataset': {'name': 'hw',\n",
       "  'data_root': './data_line/',\n",
       "  'train_annotation': 'train_line_annotation.txt',\n",
       "  'valid_annotation': 'test_line_annotation.txt',\n",
       "  'image_height': 32,\n",
       "  'image_min_width': 32,\n",
       "  'image_max_width': 512},\n",
       " 'dataloader': {'num_workers': 0, 'pin_memory': True},\n",
       " 'aug': {'image_aug': True, 'masked_language_model': True},\n",
       " 'predictor': {'beamsearch': False},\n",
       " 'quiet': False,\n",
       " 'pretrain': './ckpts/vgg_transformer.pth',\n",
       " 'weights': 'https://vocr.vn/data/vietocr/vgg_transformer.pth',\n",
       " 'backbone': 'vgg19_bn',\n",
       " 'cnn': {'pretrained': False,\n",
       "  'ss': [[2, 2], [2, 2], [2, 1], [2, 1], [1, 1]],\n",
       "  'ks': [[2, 2], [2, 2], [2, 1], [2, 1], [1, 1]],\n",
       "  'hidden': 256},\n",
       " 'slim': {'mode': 'prune'}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Cfg.load_config_from_name(\"vgg_transformer\")\n",
    "config['cnn']['pretrained']=False\n",
    "config['device'] = 'cpu'\n",
    "config['pretrain'] = \"./ckpts/vgg_transformer.pth\"\n",
    "config['dataloader']['num_workers'] = 0\n",
    "config['trainer'].update({'num_epochs': 100})\n",
    "config.update({'slim': {'mode': 'prune'}})\n",
    "\n",
    "dataset_params = {\n",
    "    \"name\": \"hw\",\n",
    "    \"data_root\": \"./data_line/\",\n",
    "    \"train_annotation\": \"train_line_annotation.txt\",\n",
    "    \"valid_annotation\": \"test_line_annotation.txt\",\n",
    "}\n",
    "config['dataset'].update(dataset_params)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd8c837e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lihardingnguyen/.miniconda3/envs/ai-env/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before pruning\n",
      "base_macs=8320363639.28125, base_params=37650217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lihardingnguyen/.miniconda3/envs/ai-env/lib/python3.11/site-packages/torch_pruning/pruner/algorithms/metapruner.py:101: UserWarning: channel_groups is deprecated. Please use in_channel_groups and out_channel_groups instead.\n",
      "  warnings.warn(\"channel_groups is deprecated. Please use in_channel_groups and out_channel_groups instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After pruning\n",
      "base_macs_after_pruning=2601952009.65625, base_params_after_pruning=13010359\n",
      "train_hw exists. Remove folder if you want to create new dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_hw build cluster: 100%|███████████████████████████████| 5482/5482 [00:00<00:00, 228844.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_hw exists. Remove folder if you want to create new dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "valid_hw build cluster: 100%|███████████████████████████████| 1812/1812 [00:00<00:00, 164084.78it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f866685c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lihardingnguyen/.miniconda3/envs/ai-env/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 000000 - train loss: nan - lr: 1.20e-05 - load time: 0.13 - gpu time: 0.00\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (128) must match the size of tensor b (129) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developments/H_Kaopiz/vietocr-slim/vietocr/model/trainer.py:129\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog(info)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_annotation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 129\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate()\n\u001b[1;32m    130\u001b[0m     acc_full_seq, acc_per_char \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics)\n\u001b[1;32m    132\u001b[0m     info \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch: \u001b[39m\u001b[38;5;132;01m{:06d}\u001b[39;00m\u001b[38;5;124m - iter: \u001b[39m\u001b[38;5;132;01m{:06d}\u001b[39;00m\u001b[38;5;124m - valid loss: \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m - acc full seq: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m - acc per char: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    133\u001b[0m         epoch,\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m         acc_per_char,\n\u001b[1;32m    138\u001b[0m     )\n",
      "File \u001b[0;32m~/Developments/H_Kaopiz/vietocr-slim/vietocr/model/trainer.py:208\u001b[0m, in \u001b[0;36mprecision\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m~/Developments/H_Kaopiz/vietocr-slim/vietocr/model/trainer.py:191\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m    189\u001b[0m     translated_sentence = batch_translate_beam_search(batch[\"img\"], self.model)\n\u001b[1;32m    190\u001b[0m     prob = None\n\u001b[0;32m--> 191\u001b[0m else:\n\u001b[1;32m    192\u001b[0m     translated_sentence, prob = translate(batch[\"img\"], self.model)\n\u001b[1;32m    194\u001b[0m pred_sent = self.vocab.batch_decode(translated_sentence.tolist())\n",
      "File \u001b[0;32m~/Developments/H_Kaopiz/vietocr-slim/vietocr/tool/translate.py:93\u001b[0m, in \u001b[0;36mtranslate\u001b[0;34m(img, model, max_seq_length, sos_token, eos_token)\u001b[0m\n\u001b[1;32m     89\u001b[0m             tgt_inp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(translated_sentence)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m#            output = model(img, tgt_inp, tgt_key_padding_mask=None)\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m#            output = model.transformer(src, tgt_inp, tgt_key_padding_mask=None)\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m             output, memory \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt_inp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m             output \u001b[38;5;241m=\u001b[39m softmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     95\u001b[0m             output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Developments/H_Kaopiz/vietocr-slim/vietocr/model/seqmodel/transformer.py:94\u001b[0m, in \u001b[0;36mLanguageTransformer.forward_decoder\u001b[0;34m(self, tgt, memory)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_decoder\u001b[39m(\u001b[38;5;28mself\u001b[39m, tgt, memory):\n\u001b[0;32m---> 94\u001b[0m     tgt_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen_nopeek_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(tgt\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     95\u001b[0m     tgt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_enc(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tgt(tgt) \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model))\n\u001b[1;32m     97\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mdecoder(tgt, memory, tgt_mask\u001b[38;5;241m=\u001b[39mtgt_mask)\n",
      "File \u001b[0;32m~/Developments/H_Kaopiz/vietocr-slim/vietocr/model/seqmodel/transformer.py:83\u001b[0m, in \u001b[0;36mLanguageTransformer.gen_nopeek_mask\u001b[0;34m(self, length)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgen_nopeek_mask\u001b[39m(\u001b[38;5;28mself\u001b[39m, length):\n\u001b[0;32m---> 83\u001b[0m     mask \u001b[38;5;241m=\u001b[39m (\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtriu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     84\u001b[0m     mask \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmasked_fill(mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mmasked_fill(mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m0.0\u001b[39m))\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mask\n",
      "File \u001b[0;32m~/Developments/H_Kaopiz/vietocr-slim/vietocr/model/seqmodel/transformer.py:20\u001b[0m, in \u001b[0;36mtriu_graph\u001b[0;34m(x, diagonal, out)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Exclude 'in_proj_weight' and 'in_proj_bias' tensors from the template\u001b[39;00m\n\u001b[1;32m     18\u001b[0m mask[:\u001b[38;5;241m2\u001b[39m, :\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (128) must match the size of tensor b (129) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a45265-4073-4512-9737-d2461c0dd92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = trainer.train_gen\n",
    "for batch in train_loader:\n",
    "    print(type(batch))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908b4385-9364-4817-b966-acd75e71b382",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = (batch['img'], batch[\"tgt_input\"], None)\n",
    "oto = OTO(model=model, dummy_input=dummy_input, strict_out_nodes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbff7096",
   "metadata": {},
   "outputs": [],
   "source": [
    "oto.visualize(out_dir=\"./cache\", view=False, display_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ec7bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch['img'].shape, batch[\"tgt_input\"].shape, batch[\"tgt_padding_mask\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace409c3-b42e-4330-837a-c80ea46be8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, m in model.transformer.named_modules():\n",
    "    if isinstance(m, nn.MultiheadAttention):\n",
    "        print(name, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21e364f-ec04-4bc1-a40a-4136eb99fe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = oto.hesso(\n",
    "    variant=\"adamw\",\n",
    "    lr=0.01,\n",
    "    weight_decay=1e-4,\n",
    "    target_group_sparsity=0.5,\n",
    "    start_pruning_step=10 * len(train_data),\n",
    "    pruning_periods=10,\n",
    "    pruning_steps=10 * len(train_data),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f8a235-a3ac-4754-bf6c-248c798eae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in model.modules():\n",
    "    if isinstance(m, torch.nn.Linear) and m.out_features == 233:\n",
    "        ignored_layers.append(m)\n",
    "        \n",
    "ignored_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe63101",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_groups = {}\n",
    "for m in model.modules():\n",
    "    if isinstance(m, nn.MultiheadAttention):\n",
    "        channel_groups[m] = m.num_heads\n",
    "channel_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3936a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterative_steps = 6\n",
    "example_inputs = (batch[\"img\"], batch[\"tgt_input\"], batch[\"tgt_padding_mask\"])\n",
    "pruner = tp.pruner.MagnitudePruner(\n",
    "    model,\n",
    "    example_inputs,\n",
    "    importance=imp,\n",
    "    global_pruning=True,\n",
    "    iterative_steps=iterative_steps,\n",
    "    pruning_ratio=0.5, # remove 50% channels, ResNet18 = {64, 128, 256, 512} => ResNet18_Half = {32, 64, 128, 256}\n",
    "    ignored_layers=ignored_layers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6196a889",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_macs, base_nparams = tp.utils.count_ops_and_params(model, example_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eb03a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(iterative_steps):\n",
    "    # Soft Pruning\n",
    "    for group in pruner.step(interactive=True):\n",
    "        for dep, idxs in group:\n",
    "            target_layer = dep.target.module\n",
    "            pruning_fn = dep.handler\n",
    "            if pruning_fn in [tp.prune_conv_in_channels, tp.prune_linear_in_channels]:\n",
    "                target_layer.weight.data[:, idxs] *= 0\n",
    "            elif pruning_fn in [tp.prune_conv_out_channels, tp.prune_linear_out_channels]:\n",
    "                target_layer.weight.data[idxs] *= 0\n",
    "                if target_layer.bias is not None:\n",
    "                    target_layer.bias.data[idxs] *= 0\n",
    "            elif pruning_fn in [tp.prune_batchnorm_out_channels]:\n",
    "                target_layer.weight.data[idxs] *= 0\n",
    "                target_layer.bias.data[idxs] *= 0\n",
    "                \n",
    "    macs, nparams = tp.utils.count_ops_and_params(model, example_inputs)\n",
    "    print(model)\n",
    "    print(model(*example_inputs).shape)\n",
    "    \n",
    "    print(\n",
    "            \"  Iter %d/%d, Params: %.2f M => %.2f M\"\n",
    "            % (i+1, iterative_steps, base_nparams / 1e6, nparams / 1e6)\n",
    "        )\n",
    "    print(\n",
    "            \"  Iter %d/%d, MACs: %.2f G => %.2f G\"\n",
    "            % (i+1, iterative_steps, base_macs / 1e9, macs / 1e9)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccaa39f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
